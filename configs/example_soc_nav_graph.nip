n_train_envs: 1
env_max_steps: &env_max_steps 200
eval_period: `40 * env_max_steps`
eval_n_episodes: &eval_n_episodes 10
peds_padding: &peds_padding 8


# Environment and problem settings
action_space: &action_space !ContinuousUnicycleActionSpace
  lb: (0., -2.826)
  ub: (2., 2.826)  # 0.9 * pi
  normalize: true
#action_space: &action_space !ContinuousPolarSubgoalActionSpace
#  lb: (0.5, -1.74533)  # TODO: Should we just stay on place if subgoal distance is less than robot radius?
#  ub: (3., 1.74533)  # 100 degrees
#  normalize: true

controller: &controller !DefaultMPCFactory
  horizon: 25
  total_peds: *peds_padding
  lb: (0., -2.)
  ub: (2., 2.)


curriculum: &curriculum !SequentialCurriculum
  agents_samplers:
#    - !RobotOnlySampler
#      sampling_square: (12, 12)
#      min_robot_goal_distance: 5.
#    - !RandomAgentsSampler
#      n_peds: (4, 8)
#      sampling_square: (12, 12)
#      min_robot_goal_distance: 5.
#      ped_linear_vels: (1., 2.3)
    - !CircularRobotCentralSampler
      n_peds: (3, 5)
      ped_circle_radius: (1.8, 3)
      ped_linear_vels: (1., 1.8)
  problem_samplers:
#    - !RandomProblemSampler
#      ped_model: "hsfm"
#      robot_visible: false
#      detector_range: 5.
#      detector_fov: 360.
#      goal_reach_threshold: 0.1
#      max_steps: 100
#      subgoal_reach_threshold: 0.1
#      max_subgoal_steps: 20
    - !RandomProblemSampler
      ped_model: "hsfm"
      robot_visible: false
      detector_range: 5.
      detector_fov: 360.
      goal_reach_threshold: 0.1
      max_steps: 200
      subgoal_reach_threshold: 0.1
      max_subgoal_steps: 50
  stages:
#    - ("no_ped", 1.)
    - ("random_ped", 0.8)
  n_eval_episodes: `eval_n_episodes + 1`  # Hack for the stable-baselines3 evaluation pipeline


# Reward function
reward: &reward !BranchReward
  success_reward: 10.
  fail_reward: -20.
  step_reward: !CompositeReward
    rewards:
      - !PotentialGoalReward
        coefficient: 2.
      - !AngularVelocityPenalty
        coefficient: 0.005
      - !BasicPredictionPenalty
        factor: 20.
      - !DiscomfortPenalty
        coefficient: 1.
  truncated_is_fail: false


# Environment
train_env_factory: !SocialNavGraphEnvFactory
  action_space_config: *action_space
  sim_config: !SimConfig
    render: true
  curriculum: *curriculum
  tracker_factory: !CovarianceNetTrackerFactory
    horizon: 25
    max_ghost_tracking_time: 8
    device: "cuda"
  rl_tracker_horizon: 5
  reward: *reward
  peds_padding: *peds_padding
#  controller_factory: *controller


# RL model
feature_extractor: !&PoolingGraphExtractor
feature_extractor_kwargs:
  embedding_dim: 128

rl_model: !&PPO
#rl_model_params:
#  policy_kwargs:
#    activation_fn: "tanh"
#    net_arch:
#      pi: [256, 128, 32]
#      vf: [256, 128, 32]


# Logging
logger: !ConsoleLogger
#logger: !NeptuneLogger
#  neptune_project: "timeescaper/soc-nav-rl"
