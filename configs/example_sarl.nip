n_train_envs: 32
eval_period: `40 * 200`
eval_n_episodes: &eval_n_episodes 10
peds_padding: &peds_padding 8


curriculum: &curriculum !SequentialCurriculum 
  agents_samplers: 
    - !RandomAgentsSampler 
      n_peds: (1, 3)
      sampling_square: (6, 6)
      min_robot_goal_distance: 0.9
      ped_linear_vels: (1.0, 1.8)

    - !CompositeAgentsSampler 
      samplers: 
        - !RandomAgentsSampler 
          n_peds: (1, 4)
          sampling_square: (6, 6)
          min_robot_goal_distance: 2.0
          ped_linear_vels: (1.0, 1.8)
        - !CircularRobotCentralSampler 
          n_peds: (2, 4)
          ped_circle_radius: (1.8, 3.5)
          ped_linear_vels: (1.0, 1.8)
        - !ParallelCrossingSampler 
          n_peds: (2, 4)
          min_robot_goal_distance: 2.0
          ped_linear_vels: (1.0, 1.8)

    - !CompositeAgentsSampler 
      samplers: 
        - !RandomAgentsSampler 
          n_peds: (4, 8)
          sampling_square: (8, 8)
          min_robot_goal_distance: 4.0
          ped_linear_vels: (1.0, 1.8)
        - !CircularRobotCentralSampler 
          n_peds: (4, 6)
          ped_circle_radius: (2, 3.5)
          ped_linear_vels: (1.0, 1.8)
        - !ParallelCrossingSampler 
          n_peds: (4, 8)
          min_robot_goal_distance: 3.0
          ped_linear_vels: (1.0, 1.8)
          max_sample_trials: 1000

  problem_samplers: 
    - !RandomProblemSampler 
      ped_model: "hsfm"
      robot_visible: False
      detector_range: 5.0
      detector_fov: 360.0
      goal_reach_threshold: 0.1
      max_steps: 100

    - !RandomProblemSampler 
      ped_model: "hsfm"
      robot_visible: False
      detector_range: 5.0
      detector_fov: 360.0
      goal_reach_threshold: 0.1
      max_steps: 100

    - !RandomProblemSampler 
      ped_model: "hsfm"
      robot_visible: False
      detector_range: 5.0
      detector_fov: 360.0
      goal_reach_threshold: 0.1
      max_steps: 120

  stages: 
    - ('intro', 0.8)
    - ('medium', 0.8)
    - ('hard', 1.0)

  n_eval_episodes: `eval_n_episodes + 1`


train_env_factory: !WrappedEnvFactory 
  sim_config: !SimConfig 
    render: False
  curriculum: *curriculum
  wrappers: 
    - !EnvWrapEntry 
      env_cls: !&UnicycleEnv 
      kwargs: 
        lb: (0.0, -2.0)
        ub: (2.0, 2.0)
        normalize: True

    - !EnvWrapEntry 
      env_cls: !&PredictionEnv 
      kwargs: 
        peds_padding: *peds_padding
        tracker_factory: !ConstantVelocityTrackerFactory 
          horizon: 7
          max_ghost_tracking_time: 8

    - !EnvWrapEntry 
      env_cls: !&SARLPredictionRewardEnv

    - !EnvWrapEntry 
      env_cls: !&SARLPredictionEnv 
      kwargs: 
        rl_horizon: 7
        peds_padding: *peds_padding

    - !EnvWrapEntry 
      env_cls: !&TimeLimitEnv


feature_extractor: !&SARLPredictionFeatureExtractor 
feature_extractor_kwargs: 
  activation: "tanh"

rl_model: !&PPO 
rl_model_params: 
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  clip_range: 0.2
  max_grad_norm: 0.5
  verbose: 1
  policy_kwargs: 
    activation_fn: "tanh"


logger: !ConsoleLogger
#logger: !NeptuneLogger
#  neptune_project: "timeescaper/soc-nav-rl"