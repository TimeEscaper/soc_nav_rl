n_train_envs: 16
eval_period: `40 * 200`
eval_n_episodes: &eval_n_episodes 10
peds_padding: &peds_padding 8


curriculum: &curriculum !SequentialCurriculum
  agents_samplers:
    - !RandomAgentsSampler
      n_peds: (1, 3)
      sampling_square: (6, 6)
      min_robot_goal_distance: 0.9
      ped_linear_vels: (1., 1.8)
    - !RandomAgentsSampler
      n_peds: (1, 4)
      sampling_square: (6, 6)
      min_robot_goal_distance: 2.
      ped_linear_vels: (1., 1.8)
    - !RandomAgentsSampler
      n_peds: (4, 8)
      sampling_square: (8, 8)
      min_robot_goal_distance: 4.
      ped_linear_vels: (1., 1.8)

  problem_samplers:
    - !RandomProblemSampler
      ped_model: "hsfm"
      robot_visible: false
      detector_range: 5.
      detector_fov: 360.
      goal_reach_threshold: 0.1
      max_steps: 100
    - !RandomProblemSampler
      ped_model: "hsfm"
      robot_visible: false
      detector_range: 5.
      detector_fov: 360.
      goal_reach_threshold: 0.1
      max_steps: 100
    - !RandomProblemSampler
      ped_model: "hsfm"
      robot_visible: false
      detector_range: 5.
      detector_fov: 360.
      goal_reach_threshold: 0.1
      max_steps: 120

  stages:
    - ("intro", 0.8)
    - ("medium", 0.8)
    - ("hard", 1.)
  n_eval_episodes: `eval_n_episodes + 1`  # Hack for the stable-baselines3 evaluation pipeline


# Environment
train_env_factory: !WrappedEnvFactory
  sim_config: !SimConfig
    render: false
  curriculum: *curriculum
  wrappers:
    - !EnvWrapEntry
      env_cls: !&UnicycleEnv
      kwargs:
        lb: (0., -2.)
        ub: (2., 2.)
        normalize: true
    - !EnvWrapEntry
      env_cls: !&PredictionEnv
      kwargs:
        peds_padding: *peds_padding
        rl_horizon: 5
        tracker_factory: !ConstantVelocityTrackerFactory
          horizon: 25
          max_ghost_tracking_time: 8
    - !EnvWrapEntry
      env_cls: !&SARLRewardEnv
    - !EnvWrapEntry
      env_cls: !&TimeLimitEnv


# RL model
feature_extractor: !&SARLPredictionFeatureExtractor
feature_extractor_kwargs:
  activation: "tanh"

rl_model: !&PPO
rl_model_params:
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  clip_range: 0.2
  max_grad_norm: 0.5
  verbose: 0
  policy_kwargs:
    activation_fn: "tanh"
#    net_arch:
#      pi: [256, 128, 32]
#      vf: [256, 128, 32]


# Logging
logger: !ConsoleLogger
#logger: !NeptuneLogger
#  neptune_project: "timeescaper/soc-nav-rl"
