n_train_envs: 16
eval_period: `40 * 15`
eval_n_episodes: &eval_n_episodes 10
peds_padding: &peds_padding 8


curriculum: &curriculum !SequentialCurriculum
  agents_samplers:
    - !RandomAgentsSampler
      n_peds: (1, 3)
      sampling_square: (6, 6)
      min_robot_goal_distance: 0.9
      ped_linear_vels: (1., 1.8)
    - !RandomAgentsSampler
      n_peds: (1, 4)
      sampling_square: (6, 6)
      min_robot_goal_distance: 2.
      ped_linear_vels: (1., 1.8)
    - !RandomAgentsSampler
      n_peds: (4, 8)
      sampling_square: (8, 8)
      min_robot_goal_distance: 4.
      ped_linear_vels: (1., 1.8)

  problem_samplers:
    - !RandomProblemSampler
      ped_model: "hsfm"
      robot_visible: false
      detector_range: 5.
      detector_fov: 360.
      goal_reach_threshold: 0.1
      max_steps: 15
    - !RandomProblemSampler
      ped_model: "hsfm"
      robot_visible: false
      detector_range: 5.
      detector_fov: 360.
      goal_reach_threshold: 0.1
      max_steps: 15
    - !RandomProblemSampler
      ped_model: "hsfm"
      robot_visible: false
      detector_range: 5.
      detector_fov: 360.
      goal_reach_threshold: 0.1
      max_steps: 15

  stages:
    - ("intro", 0.8)
    - ("medium", 0.8)
    - ("hard", 1.)
  n_eval_episodes: `eval_n_episodes + 1`  # Hack for the stable-baselines3 evaluation pipeline


# Environment
train_env_factory: !WrappedEnvFactory
  sim_config: !SimConfig
    render: false
  curriculum: *curriculum
  wrappers:
    - !EnvWrapEntry
      env_cls: !&UnicycleEnv
      kwargs:
        lb: (0., -2.)
        ub: (2., 2.)
        normalize: false
    - !EnvWrapEntry
      env_cls: !&PredictionEnv
      kwargs:
        peds_padding: *peds_padding
        tracker_factory: !CovarianceNetTrackerFactory
          horizon: 25
          max_ghost_tracking_time: 8
          device: "cuda"
    - !EnvWrapEntry
      env_cls: !&SubgoalEnv
      kwargs:
        lb: (0.7, -1.92)
        ub: (3., 1.92)
        controller_factory: !DefaultMPCFactory
          horizon: 25
          total_peds: *peds_padding
          lb: (0., -2.)
          ub: (2., 2.)
        normalize: true
        max_subgoal_steps: 25
    - !EnvWrapEntry
      env_cls: !&SARLPredictionEnv
      kwargs:
        rl_horizon: 5
        peds_padding: *peds_padding
    - !EnvWrapEntry
      env_cls: !&TimeLimitEnv
    - !EnvWrapEntry
      env_cls: !&SARLRewardEnv
      kwargs:
        step_reward: -0.01


# RL model
feature_extractor: !&SARLPredictionFeatureExtractor
feature_extractor_kwargs:
  activation: "tanh"

rl_model: !&PPO
rl_model_params:
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  clip_range: 0.2
  max_grad_norm: 0.5
  verbose: 0
  policy_kwargs:
    activation_fn: "tanh"
#    net_arch:
#      pi: [256, 128, 32]
#      vf: [256, 128, 32]


# Logging
logger: !ConsoleLogger
#logger: !NeptuneLogger
#  neptune_project: "timeescaper/soc-nav-rl"
