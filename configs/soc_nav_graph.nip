n_train_envs: 16
env_max_steps: &env_max_steps 200
eval_period: `40 * env_max_steps`
eval_n_episodes: 10


sim: &sim !SimConfig
  ped_model: "hsfm"
  robot_visible: false
  control_lb: (0., 0.)
  control_ub: (2., 2.826)  # 0.9 * pi
  goal_reach_threshold: 0.1
  max_steps: *env_max_steps


agents:  &agents !RandomAgentsSampler
  n_peds: (4, 8)
  sampling_square: (10, 10)
  min_robot_goal_distance: 4.0


reward: &reward !BranchReward
  success_reward: 10.
  fail_reward: -20.
  step_reward: !PotentialGoalReward
    coefficient: 2.
  truncated_is_fail: false


train_env_factory: !SocialNavGraphEnvFactory
  agents_sampler: *agents
  tracker_factory: !CovarianceNetTrackerFactory
    horizon: 5
    max_ghost_tracking_time: 3
    device: "cuda"
  reward: *reward
  sim_config: *sim
  normalize_actions: true
  render: false


feature_extractor: !&BasicGraphExtractor

rl_model: !&PPO
rl_model_params:
  learning_rate: 4e-5
  n_steps: 30
  n_epochs: 10
  gamma: 0.99


logger: !NeptuneLogger
  neptune_project: "timeescaper/soc-nav-rl"
